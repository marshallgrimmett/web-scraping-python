{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% || Number of records processed: 11084 || Number of missing parts: 1723 || Estimated time: 55437.05 seconds || Time elapsed: 55435.09 seconds || Time remaining: 1.96 seconds || Current part: ZXS1B3-500 || Error count: 2364\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import urllib.parse\n",
    "\n",
    "def printProgress(progress, i, missingParts, avgTime, timeRemain, total_time, currPart, errorCount):\n",
    "    clear_output(wait=True)\n",
    "    print('Progress: ' + str(progress) + '% || ' +\n",
    "          'Number of records processed: ' + str(i) + ' || ' +\n",
    "          'Number of missing parts: ' + str(len(missingParts)) + ' || ' +\n",
    "          'Estimated time: ' + str(avgTime) + ' seconds || ' +\n",
    "          'Time elapsed: ' + str(total_time) + ' seconds' + ' || ' +\n",
    "          'Time remaining: ' + str(timeRemain) + ' seconds || ' +\n",
    "          'Current part: ' + currPart + ' || ' +\n",
    "          'Error count: ' + str(errorCount))\n",
    "\n",
    "# Get parts list\n",
    "partsDf = pd.read_excel('data\\\\45292-parts.xlsx', sheet_name='Sheet1')\n",
    "partsList = partsDf['Prcpart']\n",
    "\n",
    "# drop duplicates\n",
    "partsList = list(dict.fromkeys(partsList.tolist()))\n",
    "\n",
    "# DataFrames\n",
    "detailsDf = pd.DataFrame() # dataframe to store all info on each part\n",
    "popDf = pd.DataFrame() # store popularity data\n",
    "histDf = pd.DataFrame() # store stock and price history\n",
    "distDf = pd.DataFrame() # store distributors with stock\n",
    "\n",
    "# keep track of index in df\n",
    "idx = 0\n",
    "\n",
    "# keep track of parts that were not found\n",
    "missingParts = []\n",
    "\n",
    "# log all errors\n",
    "log = []\n",
    "\n",
    "# Used for computing timers and progress trackers\n",
    "startTime = time.time()\n",
    "total_time = 0\n",
    "estimatedTime = []\n",
    "\n",
    "# Url for searching\n",
    "url = 'https://www.findchips.com'\n",
    "search = '/detail/'\n",
    "\n",
    "# Loop through each part in the part list provided\n",
    "for i, part in enumerate(partsList):\n",
    "    try:\n",
    "        begin_time = time.time()\n",
    "\n",
    "        # if the part starts with \"CON\" remove it\n",
    "        if (part[:3].lower() == 'con') or (part[:3].lower() == 'stk'):\n",
    "            part = part[3:]\n",
    "\n",
    "        # convert to url encoding\n",
    "        partParsed = urllib.parse.quote(part, safe='')\n",
    "\n",
    "        # get the web page\n",
    "        page = requests.get(url + search + partParsed)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        mfg = soup.find('div', class_='analytics-top-distributor')\n",
    "\n",
    "        # Basically, if there is no mfg, then there is no data.\n",
    "        # if there is mfg, then we need to see if it is a list or if it is just a single mfg.\n",
    "        if mfg is None:\n",
    "            mfgList = []\n",
    "            missingParts.append(part)\n",
    "        else:\n",
    "            mfgList = mfg.find('select', class_='j-select-manufacturer')\n",
    "            if mfgList is None:\n",
    "                mfgList = mfg.find('span')\n",
    "            else:\n",
    "                mfgList = mfgList.find_all('option')\n",
    "\n",
    "        # Loop through each mfg for that part\n",
    "        for mfg in mfgList:\n",
    "\n",
    "            if isinstance(mfg, bs4.element.Tag) and mfg.has_attr('value'):\n",
    "                param = mfg['value']\n",
    "                page = requests.get(url + param)\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # Analytics top header\n",
    "            # to do\n",
    "\n",
    "            # Part info\n",
    "            results = soup.find_all('li', class_='part-details-list-item')\n",
    "            if results is not None:\n",
    "                for item in results:\n",
    "                    if item is not None:\n",
    "                        detail = item.find('small').text.strip()\n",
    "                        value = item.find('p').text.strip()\n",
    "                        detailsDf.loc[idx, detail] = value\n",
    "\n",
    "            # CAD models information\n",
    "            # to do\n",
    "\n",
    "            # Distributors with stock\n",
    "            results = soup.find('div', class_='total-inventory-right j-pagination-holder')\n",
    "            if results is not None:\n",
    "                results = results.find_all('li', class_='even-odd-list-item')\n",
    "                distDf.loc[idx, 'part'] = part\n",
    "                for item in results:\n",
    "                    count = item.find('span', class_='count').text.strip()\n",
    "                    distName = item.find('span', class_='distri-name').find('a').text.strip()\n",
    "                    value = item.find('span', class_='inventory-number').text.strip()\n",
    "                    distDf.loc[idx, distName] = value\n",
    "\n",
    "            # Equivalents\n",
    "            # to do\n",
    "\n",
    "            # Popularity by part\n",
    "            results = soup.find_all('div', class_='dash-section col-xs')\n",
    "            if results is not None:\n",
    "                for item in results:\n",
    "                    title = item.find('h4').text.strip()\n",
    "                    rank = item.find('p', class_='popularity-title').find('strong').text.strip()\n",
    "                    total = item.find('p', class_='popularity-title').text.strip()\n",
    "                    total = total[total.index('of') + len('of'):total.index('parts')].strip()\n",
    "                    change = item.find('span').text.strip()\n",
    "                    popGraph = item.find('div', class_='popularity-graph-global')                           # not sure how to parse this yet\n",
    "                    popDf.loc[idx, title] = int(rank.replace(',', ''))\n",
    "                    popDf.loc[idx, title + ' total'] = int(total.replace(',', ''))\n",
    "\n",
    "            # The last column has a different class so we must select it seperately\n",
    "            results = soup.find('div', class_='dash-section col-xs last')\n",
    "            if results is not None:\n",
    "                title = results.find('h4').text.strip()\n",
    "                rank = results.find('p', class_='popularity-title').find('strong').text.strip()\n",
    "                total = results.find('p', class_='popularity-title').text.strip()\n",
    "                total = total[total.index('of') + len('of'):total.index('parts')].strip()\n",
    "                change = results.find('span').text.strip()\n",
    "                popGraph = results.find('div', class_='popularity-graph-global')                           # not sure how to parse this yet\n",
    "                popDf.loc[idx, title] = int(rank.replace(',', ''))\n",
    "                popDf.loc[idx, title + ' total'] = int(total.replace(',', ''))\n",
    "\n",
    "            # Popularity by region\n",
    "            # to do\n",
    "\n",
    "            # Estimated price history\n",
    "            # to do\n",
    "\n",
    "            # Estimated stock history\n",
    "            # to do\n",
    "\n",
    "            # Market price analysis\n",
    "            # below code does not work because content seems to be loaded dynamically.\n",
    "    #         results = soup.find_all('span', class_='j-percent percentage')\n",
    "    #         if results is not None:\n",
    "    #             for i, item in enumerate(results):\n",
    "    #                 percent = item.text.strip()\n",
    "    #                 if i == 0:\n",
    "    #                     column = 'price'\n",
    "    #                 else:\n",
    "    #                     column = 'stock'\n",
    "    #                 histDf.loc[idx, title] = float(percent.strip('%'))/100\n",
    "\n",
    "            # Datasheets\n",
    "            # to do\n",
    "\n",
    "            # Related parts\n",
    "            # to do\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        log.append([part, i, e])\n",
    "\n",
    "    # calculate progress trackers\n",
    "    progress = round((i+1)/len(partsList)*100, 2)\n",
    "    estimatedTime.append(round((len(partsList) * (time.time() - begin_time)), 2))\n",
    "    avgTime = round((sum(estimatedTime) / len(estimatedTime)), 2)\n",
    "    total_time = round((total_time + time.time() - begin_time), 2)\n",
    "    timeRemain = round((avgTime - total_time), 2)\n",
    "    printProgress(progress, (i+1), missingParts, avgTime, timeRemain, total_time, part, len(log))\n",
    "\n",
    "# Save\n",
    "pd.DataFrame(log).to_csv('web_scraped_log2.csv', encoding='utf_8_sig')\n",
    "pd.DataFrame(missingParts).to_csv('web_scraped_missing_parts2.csv', encoding='utf_8_sig')\n",
    "detailsDf.to_csv('web_scraped_details2.csv', encoding='utf_8_sig')\n",
    "popDf.to_csv('web_scraped_popularity2.csv', encoding='utf_8_sig')\n",
    "histDf.to_csv('web_scraped_history2.csv', encoding='utf_8_sig')\n",
    "distDf.to_csv('web_scraped_distributors2.csv', encoding='utf_8_sig')\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "# Progress: 100.0% || Number of records processed: 11084 || Number of missing parts: 2154 || Estimated time: 77664.13 seconds || Time elapsed: 77666.88 seconds || Time remaining: -2.75 seconds || Current part: ZXS1B3-500 || Error count: 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logDf = pd.read_csv('web_scraped_log.csv')\n",
    "infoDf = pd.read_csv('web_scraped_parts_info_2.csv')\n",
    "# missingDf = pd.read_csv('web_scraped_missing_parts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1567"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "popCols = [col for col in infoDf.columns if 'Popularity' in col]\n",
    "popDf = infoDf[popCols]\n",
    "infoDf = infoDf.drop(popCols, axis=1)\n",
    "infoDf.to_csv('web_scraped_details.csv', encoding='utf_8_sig')\n",
    "popDf.to_csv('web_scraped_popularity.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailsDf = pd.read_csv('web_scraped_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'Manufacturer Part Number:', 'Rohs Code:',\n",
       "       'Part Life Cycle Code:', 'Ihs Manufacturer:', 'Package Description:',\n",
       "       'Reach Compliance Code:', 'Manufacturer:', 'Risk Rank:',\n",
       "       ...\n",
       "       'Input Connector:', 'Output Connector:', 'Output Power-Max:',\n",
       "       'Number of Signal Lines:', 'Video Standard:', 'Supply Current2-Max:',\n",
       "       'Supply Voltage2-Nom:', 'Carrier Type:', 'Carrier Type (2):',\n",
       "       'Carrier Type (3):'],\n",
       "      dtype='object', length=1056)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailsDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                             0\n",
      "Unnamed: 0.1                           0\n",
      "Manufacturer Part Number:           1662\n",
      "Is Samacsys:                        1662\n",
      "Part Life Cycle Code:               1662\n",
      "                                   ...  \n",
      "Audio Type:                        25177\n",
      "Peak Primary Current:              25177\n",
      "Horizontal Pixel Count:            25177\n",
      "Battery End of Life Indication:    25177\n",
      "Carrier Type (3):                  25177\n",
      "Length: 1056, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "detailsDf = detailsDf.replace('', np.nan)\n",
    "nullCols = detailsDf.isna().sum()\n",
    "nullCols.sort_values(inplace=True)\n",
    "print(nullCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popDf = pd.read_csv('output\\\\web_scraped_popularity.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code will get related parts if there is no data on the current search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     while(True):\n",
    "#         soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#         mfg = soup.find('div', class_='analytics-top-distributor')\n",
    "\n",
    "#         # Basically, if there is no mfg, then there is no data.\n",
    "#         # if there is mfg, then we need to see if it is a list or if it is just a single mfg.\n",
    "#         if mfg is None:\n",
    "#             mfgList = []\n",
    "#         else:\n",
    "#             mfgList = mfg.find('select', class_='j-select-manufacturer')\n",
    "#             if mfgList is None:\n",
    "#                 mfgList = mfg.find('span')\n",
    "#             else:\n",
    "#                 mfgList = mfgList.find_all('option')\n",
    "\n",
    "#         # if mfgList is empty, try one of the related parts\n",
    "#         if not mfgList:\n",
    "#             temp = soup.find_all('li', class_='part-details-list-item')\n",
    "#             if len(temp) > 1:\n",
    "#                 temp = temp[1].find('a')\n",
    "#                 page = requests.get(url + temp['href'])\n",
    "#             else:\n",
    "#                 break\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     if not mfgList:\n",
    "#         missingParts.append(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import urllib.parse\n",
    "\n",
    "def printProgress(progress, i, missingParts, avgTime, timeRemain, total_time, currPart, errorCount):\n",
    "    clear_output(wait=True)\n",
    "    print('Progress: ' + str(progress) + '% || ' +\n",
    "          'Number of records processed: ' + str(i) + ' || ' +\n",
    "          'Number of missing parts: ' + str(len(missingParts)) + ' || ' +\n",
    "          'Estimated time: ' + str(avgTime) + ' seconds || ' +\n",
    "          'Time elapsed: ' + str(total_time) + ' seconds' + ' || ' +\n",
    "          'Time remaining: ' + str(timeRemain) + ' seconds || ' +\n",
    "          'Current part: ' + currPart + ' || ' +\n",
    "          'Error count: ' + str(errorCount))\n",
    "\n",
    "part = 'CRCW02013K00FKED'\n",
    "# part = '83619-9011'\n",
    "# part = '1N4148W-7-F'       # <class 'bs4.element.Tag'>\n",
    "# part = '83619-9011'      # <class 'bs4.element.NavigableString'>\n",
    "\n",
    "# DataFrames\n",
    "detailsDf = pd.DataFrame() # dataframe to store all info on each part\n",
    "popDf = pd.DataFrame() # store popularity data\n",
    "histDf = pd.DataFrame() # store stock and price history\n",
    "distDf = pd.DataFrame() # store distributors with stock\n",
    "\n",
    "# keep track of index in df\n",
    "idx = 0\n",
    "\n",
    "# Url for searching\n",
    "url = 'https://www.findchips.com'\n",
    "search = '/detail/'\n",
    "\n",
    "# if the part starts with \"CON\" remove it\n",
    "if (part[:3].lower() == 'con') or (part[:3].lower() == 'stk'):\n",
    "    part = part[3:]\n",
    "\n",
    "# convert to url encoding\n",
    "partParsed = urllib.parse.quote(part, safe='')\n",
    "\n",
    "# get the web page\n",
    "page = requests.get(url + search + partParsed)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "mfg = soup.find('div', class_='analytics-top-distributor')\n",
    "\n",
    "# Basically, if there is no mfg, then there is no data.\n",
    "# if there is mfg, then we need to see if it is a list or if it is just a single mfg.\n",
    "if mfg is None:\n",
    "    mfgList = []\n",
    "    missingParts.append(part)\n",
    "else:\n",
    "    mfgList = mfg.find('select', class_='j-select-manufacturer')\n",
    "    if mfgList is None:\n",
    "        mfgList = mfg.find('span')\n",
    "    else:\n",
    "        mfgList = mfgList.find_all('option')\n",
    "\n",
    "\n",
    "# Loop through each mfg for that part\n",
    "for mfg in mfgList:\n",
    "\n",
    "    if isinstance(mfg, bs4.element.Tag) and mfg.has_attr('value'):\n",
    "        param = mfg['value']\n",
    "        page = requests.get(url + param)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Part info\n",
    "    results = soup.find_all('li', class_='part-details-list-item')\n",
    "    if results is not None:\n",
    "        for item in results:\n",
    "#             print(item)\n",
    "            detail = item.find('small').text.strip()\n",
    "            value = item.find('p').text.strip()\n",
    "            detailsDf.loc[idx, detail] = value\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
